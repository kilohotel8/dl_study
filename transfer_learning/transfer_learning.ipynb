{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden1_size = 3\n",
    "hidden2_size = 2\n",
    "ae_lr = 0.001\n",
    "ft_lr = 0.001\n",
    "num_epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(pd.read_csv('iris_training.csv'))\n",
    "train_target = train[:,4]\n",
    "train = train[:,:4]\n",
    "test = np.array(pd.read_csv('iris_test.csv'))\n",
    "test_target = test[:,4]\n",
    "test = test[:,:4]\n",
    "\n",
    "train_target = tf.one_hot(train_target, 3)\n",
    "test_target = tf.one_hot(test_target, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## autoencoder\n",
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "y = tf.placeholder(tf.float32, [None, 3])   \n",
    "\n",
    "def autoencoder(x):\n",
    "    # encoding\n",
    "    w1 = tf.Variable(tf.random_normal([input_size, hidden1_size]))   \n",
    "    b1 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    h1 = tf.nn.sigmoid(tf.matmul(x, w1) +b1)\n",
    "    w2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([hidden2_size]))\n",
    "    h2 = tf.nn.sigmoid(tf.matmul(h1, w2) +b2)\n",
    "    # decoding\n",
    "    w3 = tf.Variable(tf.random_normal([hidden2_size, hidden1_size]))\n",
    "    b3 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    h3 = tf.nn.sigmoid(tf.matmul(h2, w3) +b3)\n",
    "    wo = tf.Variable(tf.random_normal([hidden1_size, input_size]))\n",
    "    bo = tf.Variable(tf.random_normal([input_size]))\n",
    "    x_reconstructed = tf.nn.sigmoid(tf.matmul(h3, wo) + bo)\n",
    "    return x_reconstructed, h2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax\n",
    "def softmax_clf(x):\n",
    "    w_softmax = tf.Variable(tf.zeros([hidden2_size, 3]))\n",
    "    b_softmax = tf.Variable(tf.zeros([3]))\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x, w_softmax) + b_softmax)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred, z = autoencoder(x)\n",
    "y_pred = softmax_clf(z)\n",
    "## pretraining\n",
    "ae_loss = tf.reduce_mean(tf.pow(x - x_pred, 2))\n",
    "ae_train_step = tf.train.AdamOptimizer(ae_lr).minimize(ae_loss)\n",
    "## finetuning\n",
    "finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred)))\n",
    "finetuning_train_step = tf.train.GradientDescentOptimizer(ft_lr).minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretraining -> epoch: 0, loss: 12.433703422546387\n",
      "pretraining -> epoch: 1000, loss: 10.313177108764648\n",
      "pretraining -> epoch: 2000, loss: 10.106502532958984\n",
      "pretraining -> epoch: 3000, loss: 10.062602043151855\n",
      "pretraining -> epoch: 4000, loss: 10.04568862915039\n",
      "pretraining -> epoch: 5000, loss: 10.037540435791016\n",
      "pretraining -> epoch: 6000, loss: 10.033167839050293\n",
      "pretraining -> epoch: 7000, loss: 10.030682563781738\n",
      "pretraining -> epoch: 8000, loss: 10.029219627380371\n",
      "pretraining -> epoch: 9000, loss: 10.028345108032227\n",
      "pretraining -> epoch: 10000, loss: 10.027816772460938\n",
      "pretraining -> epoch: 11000, loss: 10.027490615844727\n",
      "pretraining -> epoch: 12000, loss: 10.027295112609863\n",
      "pretraining -> epoch: 13000, loss: 10.027172088623047\n",
      "pretraining -> epoch: 14000, loss: 10.027095794677734\n",
      "pretraining -> epoch: 15000, loss: 10.02705192565918\n",
      "pretraining -> epoch: 16000, loss: 10.027022361755371\n",
      "pretraining -> epoch: 17000, loss: 10.027006149291992\n",
      "pretraining -> epoch: 18000, loss: 10.026995658874512\n",
      "pretraining -> epoch: 19000, loss: 10.026988983154297\n",
      "pretraining -> epoch: 20000, loss: 10.026986122131348\n",
      "pretraining -> epoch: 21000, loss: 10.026983261108398\n",
      "pretraining -> epoch: 22000, loss: 10.02698040008545\n",
      "pretraining -> epoch: 23000, loss: 10.02698040008545\n",
      "pretraining -> epoch: 24000, loss: 10.02698040008545\n",
      "pretraining -> epoch: 25000, loss: 10.026979446411133\n",
      "pretraining -> epoch: 26000, loss: 10.026979446411133\n",
      "pretraining -> epoch: 27000, loss: 10.026979446411133\n",
      "pretraining -> epoch: 28000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 29000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 30000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 31000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 32000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 33000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 34000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 35000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 36000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 37000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 38000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 39000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 40000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 41000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 42000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 43000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 44000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 45000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 46000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 47000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 48000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 49000, loss: 10.026978492736816\n",
      "Pretaining finished\n",
      "finetuning -> epoch: 0, loss: 131.83343505859375\n",
      "finetuning -> epoch: 1000, loss: 131.52816772460938\n",
      "finetuning -> epoch: 2000, loss: 131.52752685546875\n",
      "finetuning -> epoch: 3000, loss: 131.52685546875\n",
      "finetuning -> epoch: 4000, loss: 131.52606201171875\n",
      "finetuning -> epoch: 5000, loss: 131.52505493164062\n",
      "finetuning -> epoch: 6000, loss: 131.5235595703125\n",
      "finetuning -> epoch: 7000, loss: 131.52093505859375\n",
      "finetuning -> epoch: 8000, loss: 131.51486206054688\n",
      "finetuning -> epoch: 9000, loss: 131.48130798339844\n",
      "finetuning -> epoch: 10000, loss: 69.08561706542969\n",
      "finetuning -> epoch: 11000, loss: 49.7178955078125\n",
      "finetuning -> epoch: 12000, loss: 41.42572784423828\n",
      "finetuning -> epoch: 13000, loss: 40.491920471191406\n",
      "finetuning -> epoch: 14000, loss: 28.14657974243164\n",
      "finetuning -> epoch: 15000, loss: 19.159229278564453\n",
      "finetuning -> epoch: 16000, loss: 12.421335220336914\n",
      "finetuning -> epoch: 17000, loss: 9.557573318481445\n",
      "finetuning -> epoch: 18000, loss: 8.127284049987793\n",
      "finetuning -> epoch: 19000, loss: 7.298213958740234\n",
      "finetuning -> epoch: 20000, loss: 6.7353973388671875\n",
      "finetuning -> epoch: 21000, loss: 6.294997215270996\n",
      "finetuning -> epoch: 22000, loss: 5.906134605407715\n",
      "finetuning -> epoch: 23000, loss: 6.141843795776367\n",
      "finetuning -> epoch: 24000, loss: 6.253658294677734\n",
      "finetuning -> epoch: 25000, loss: 6.2529497146606445\n",
      "finetuning -> epoch: 26000, loss: 6.204110145568848\n",
      "finetuning -> epoch: 27000, loss: 6.137709140777588\n",
      "finetuning -> epoch: 28000, loss: 6.063986301422119\n",
      "finetuning -> epoch: 29000, loss: 5.987064361572266\n",
      "finetuning -> epoch: 30000, loss: 5.909071445465088\n",
      "finetuning -> epoch: 31000, loss: 5.831370830535889\n",
      "finetuning -> epoch: 32000, loss: 5.754368782043457\n",
      "finetuning -> epoch: 33000, loss: 5.678740978240967\n",
      "finetuning -> epoch: 34000, loss: 5.604537487030029\n",
      "finetuning -> epoch: 35000, loss: 5.531965255737305\n",
      "finetuning -> epoch: 36000, loss: 5.461087703704834\n",
      "finetuning -> epoch: 37000, loss: 5.391872406005859\n",
      "finetuning -> epoch: 38000, loss: 5.324223041534424\n",
      "finetuning -> epoch: 39000, loss: 5.258209228515625\n",
      "finetuning -> epoch: 40000, loss: 5.193813323974609\n",
      "finetuning -> epoch: 41000, loss: 5.130932807922363\n",
      "finetuning -> epoch: 42000, loss: 5.069565296173096\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        _, pt_loss = sess.run([ae_train_step, ae_loss], feed_dict={x:train})\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"pretraining -> epoch: {}, loss: {}\".format(epoch, pt_loss))\n",
    "    print(\"Pretaining finished\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        _, ft_loss = sess.run([finetuning_train_step, finetuning_loss], feed_dict={x:train, y:sess.run(train_target)})\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"finetuning -> epoch: {}, loss: {}\".format(epoch, ft_loss))\n",
    "    print(\"fine-tuning finished\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"accuracy : {}\".format(sess.run(accuracy, feed_dict={x: test, y: sess.run(test_target)})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
