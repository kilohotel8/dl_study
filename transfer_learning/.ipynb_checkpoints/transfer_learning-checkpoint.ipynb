{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden1_size = 3\n",
    "hidden2_size = 2\n",
    "ae_lr = 0.001\n",
    "ft_lr = 0.001\n",
    "num_epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(pd.read_csv('iris_training.csv'))\n",
    "train_target = train[:,4]\n",
    "train = train[:,:4]\n",
    "test = np.array(pd.read_csv('iris_test.csv'))\n",
    "test_target = test[:,4]\n",
    "test = test[:,:4]\n",
    "\n",
    "train_target = tf.one_hot(train_target, 3)\n",
    "test_target = tf.one_hot(test_target, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## autoencoder\n",
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "y = tf.placeholder(tf.float32, [None, 3])   \n",
    "\n",
    "def autoencoder(x):\n",
    "    # encoding\n",
    "    w1 = tf.Variable(tf.random_normal([input_size, hidden1_size]))   \n",
    "    b1 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    h1 = tf.nn.sigmoid(tf.matmul(x, w1) +b1)\n",
    "    w2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([hidden2_size]))\n",
    "    h2 = tf.nn.sigmoid(tf.matmul(h1, w2) +b2)\n",
    "    # decoding\n",
    "    w3 = tf.Variable(tf.random_normal([hidden2_size, hidden1_size]))\n",
    "    b3 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    h3 = tf.nn.sigmoid(tf.matmul(h2, w3) +b3)\n",
    "    wo = tf.Variable(tf.random_normal([hidden1_size, input_size]))\n",
    "    bo = tf.Variable(tf.random_normal([input_size]))\n",
    "    x_reconstructed = tf.nn.sigmoid(tf.matmul(h3, wo) + bo)\n",
    "    return x_reconstructed, h2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax\n",
    "def softmax_clf(x):\n",
    "    w_softmax = tf.Variable(tf.zeros([hidden2_size, 3]))\n",
    "    b_softmax = tf.Variable(tf.zeros([3]))\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x, w_softmax) + b_softmax)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred, z = autoencoder(x)\n",
    "y_pred = softmax_clf(z)\n",
    "## pretraining\n",
    "ae_loss = tf.reduce_mean(tf.pow(x - x_pred, 2))\n",
    "ae_train_step = tf.train.AdamOptimizer(ae_lr).minimize(ae_loss)\n",
    "## finetuning\n",
    "finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred)))\n",
    "finetuning_train_step = tf.train.GradientDescentOptimizer(ft_lr).minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretraining -> epoch: 0, loss: 12.578307151794434\n",
      "pretraining -> epoch: 1000, loss: 10.443404197692871\n",
      "pretraining -> epoch: 2000, loss: 10.132080078125\n",
      "pretraining -> epoch: 3000, loss: 10.06744384765625\n",
      "pretraining -> epoch: 4000, loss: 10.047263145446777\n",
      "pretraining -> epoch: 5000, loss: 10.038265228271484\n",
      "pretraining -> epoch: 6000, loss: 10.033571243286133\n",
      "pretraining -> epoch: 7000, loss: 10.030922889709473\n",
      "pretraining -> epoch: 8000, loss: 10.029367446899414\n",
      "pretraining -> epoch: 9000, loss: 10.028437614440918\n",
      "pretraining -> epoch: 10000, loss: 10.027872085571289\n",
      "pretraining -> epoch: 11000, loss: 10.027525901794434\n",
      "pretraining -> epoch: 12000, loss: 10.027316093444824\n",
      "pretraining -> epoch: 13000, loss: 10.027188301086426\n",
      "pretraining -> epoch: 14000, loss: 10.027108192443848\n",
      "pretraining -> epoch: 15000, loss: 10.027053833007812\n",
      "pretraining -> epoch: 16000, loss: 10.027026176452637\n",
      "pretraining -> epoch: 17000, loss: 10.027006149291992\n",
      "pretraining -> epoch: 18000, loss: 10.026996612548828\n",
      "pretraining -> epoch: 19000, loss: 10.026989936828613\n",
      "pretraining -> epoch: 20000, loss: 10.026986122131348\n",
      "pretraining -> epoch: 21000, loss: 10.026982307434082\n",
      "pretraining -> epoch: 22000, loss: 10.026982307434082\n",
      "pretraining -> epoch: 23000, loss: 10.026981353759766\n",
      "pretraining -> epoch: 24000, loss: 10.02698040008545\n",
      "pretraining -> epoch: 25000, loss: 10.02698040008545\n",
      "pretraining -> epoch: 26000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 27000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 28000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 29000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 30000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 31000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 32000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 33000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 34000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 35000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 36000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 37000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 38000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 39000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 40000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 41000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 42000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 43000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 44000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 45000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 46000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 47000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 48000, loss: 10.026978492736816\n",
      "pretraining -> epoch: 49000, loss: 10.026978492736816\n",
      "Pretaining finished\n",
      "finetuning -> epoch: 0, loss: 131.83343505859375\n",
      "finetuning -> epoch: 1000, loss: 131.5228271484375\n",
      "finetuning -> epoch: 2000, loss: 131.50082397460938\n",
      "finetuning -> epoch: 3000, loss: 87.2593994140625\n",
      "finetuning -> epoch: 4000, loss: 56.92405319213867\n",
      "finetuning -> epoch: 5000, loss: 47.50291442871094\n",
      "finetuning -> epoch: 6000, loss: 39.203369140625\n",
      "finetuning -> epoch: 7000, loss: 20.89956283569336\n",
      "finetuning -> epoch: 8000, loss: 14.02040958404541\n",
      "finetuning -> epoch: 9000, loss: 10.685157775878906\n",
      "finetuning -> epoch: 10000, loss: 9.238019943237305\n",
      "finetuning -> epoch: 11000, loss: 8.367814064025879\n",
      "finetuning -> epoch: 12000, loss: 7.817645072937012\n",
      "finetuning -> epoch: 13000, loss: 7.496984481811523\n",
      "finetuning -> epoch: 14000, loss: 7.2709856033325195\n",
      "finetuning -> epoch: 15000, loss: 7.04976224899292\n",
      "finetuning -> epoch: 16000, loss: 6.792743682861328\n",
      "finetuning -> epoch: 17000, loss: 6.5000081062316895\n",
      "finetuning -> epoch: 18000, loss: 6.212376594543457\n",
      "finetuning -> epoch: 19000, loss: 5.9653215408325195\n",
      "finetuning -> epoch: 20000, loss: 5.763589859008789\n",
      "finetuning -> epoch: 21000, loss: 5.598803520202637\n",
      "finetuning -> epoch: 22000, loss: 5.461612224578857\n",
      "finetuning -> epoch: 23000, loss: 5.345075607299805\n",
      "finetuning -> epoch: 24000, loss: 5.244004726409912\n",
      "finetuning -> epoch: 25000, loss: 5.154685974121094\n",
      "finetuning -> epoch: 26000, loss: 5.074372291564941\n",
      "finetuning -> epoch: 27000, loss: 5.00142240524292\n",
      "finetuning -> epoch: 28000, loss: 4.934060096740723\n",
      "finetuning -> epoch: 29000, loss: 4.871878623962402\n",
      "finetuning -> epoch: 30000, loss: 4.813674449920654\n",
      "finetuning -> epoch: 31000, loss: 4.759084224700928\n",
      "finetuning -> epoch: 32000, loss: 4.707520961761475\n",
      "finetuning -> epoch: 33000, loss: 4.658409118652344\n",
      "finetuning -> epoch: 34000, loss: 4.611879348754883\n",
      "finetuning -> epoch: 35000, loss: 4.5674896240234375\n",
      "finetuning -> epoch: 36000, loss: 4.522772789001465\n",
      "finetuning -> epoch: 37000, loss: 4.502270698547363\n",
      "finetuning -> epoch: 38000, loss: 4.115513801574707\n",
      "finetuning -> epoch: 39000, loss: 4.42401123046875\n",
      "finetuning -> epoch: 40000, loss: 4.649247169494629\n",
      "finetuning -> epoch: 41000, loss: 4.56046724319458\n",
      "finetuning -> epoch: 42000, loss: 4.177354335784912\n",
      "finetuning -> epoch: 43000, loss: 4.070531845092773\n",
      "finetuning -> epoch: 44000, loss: 4.4456787109375\n",
      "finetuning -> epoch: 45000, loss: 4.171426773071289\n",
      "finetuning -> epoch: 46000, loss: 4.1402177810668945\n",
      "finetuning -> epoch: 47000, loss: 4.146321773529053\n",
      "finetuning -> epoch: 48000, loss: 4.1215996742248535\n",
      "finetuning -> epoch: 49000, loss: 4.094910144805908\n",
      "finetuning -> epoch: 50000, loss: 4.06900691986084\n",
      "finetuning -> epoch: 51000, loss: 4.044137954711914\n",
      "finetuning -> epoch: 52000, loss: 4.019721508026123\n",
      "finetuning -> epoch: 53000, loss: 3.996265172958374\n",
      "finetuning -> epoch: 54000, loss: 3.9733967781066895\n",
      "finetuning -> epoch: 55000, loss: 3.9513463973999023\n",
      "finetuning -> epoch: 56000, loss: 3.9299330711364746\n",
      "finetuning -> epoch: 57000, loss: 3.9091861248016357\n",
      "finetuning -> epoch: 58000, loss: 3.8891730308532715\n",
      "finetuning -> epoch: 59000, loss: 3.8697595596313477\n",
      "finetuning -> epoch: 60000, loss: 3.8510382175445557\n",
      "finetuning -> epoch: 61000, loss: 3.8327276706695557\n",
      "finetuning -> epoch: 62000, loss: 3.815150737762451\n",
      "finetuning -> epoch: 63000, loss: 3.798131227493286\n",
      "finetuning -> epoch: 64000, loss: 3.781736373901367\n",
      "finetuning -> epoch: 65000, loss: 3.7657644748687744\n",
      "finetuning -> epoch: 66000, loss: 3.7504024505615234\n",
      "finetuning -> epoch: 67000, loss: 3.7355144023895264\n",
      "finetuning -> epoch: 68000, loss: 3.721255302429199\n",
      "finetuning -> epoch: 69000, loss: 3.7075016498565674\n",
      "finetuning -> epoch: 70000, loss: 3.6943578720092773\n",
      "finetuning -> epoch: 71000, loss: 3.681882381439209\n",
      "finetuning -> epoch: 72000, loss: 3.6700549125671387\n",
      "finetuning -> epoch: 73000, loss: 3.659130096435547\n",
      "finetuning -> epoch: 74000, loss: 3.6491434574127197\n",
      "finetuning -> epoch: 75000, loss: 3.640529155731201\n",
      "finetuning -> epoch: 76000, loss: 3.6338374614715576\n",
      "finetuning -> epoch: 77000, loss: 3.6293087005615234\n",
      "finetuning -> epoch: 78000, loss: 3.627434015274048\n",
      "finetuning -> epoch: 79000, loss: 3.6276299953460693\n",
      "fine-tuning finished\n",
      "accuracy : 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        _, pt_loss = sess.run([ae_train_step, ae_loss], feed_dict={x:train})\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"pretraining -> epoch: {}, loss: {}\".format(epoch, pt_loss))\n",
    "    print(\"Pretaining finished\")\n",
    "    \n",
    "    for epoch in range(num_epochs+30000):\n",
    "        _, ft_loss = sess.run([finetuning_train_step, finetuning_loss], feed_dict={x:train, y:sess.run(train_target)})\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"finetuning -> epoch: {}, loss: {}\".format(epoch, ft_loss))\n",
    "    print(\"fine-tuning finished\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"accuracy : {}\".format(sess.run(accuracy, feed_dict={x: test, y: sess.run(test_target)})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
